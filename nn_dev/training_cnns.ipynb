{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spolivin/cifar10-website/blob/master/nn_dev/training_cnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Computer Vision models (CIFAR 10 Dataset)"
      ],
      "metadata": {
        "id": "YrvnUECVtRHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is dedicated to creating and training the model that would perform well on CIFAR-10 dataset. The resulting weights of the best model will be later uploaded to HuggingFace and used for converting the model to ONNX format."
      ],
      "metadata": {
        "id": "xRSfbSw4tZD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the environment"
      ],
      "metadata": {
        "id": "x0Y19fwXn1YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As is well known, when training and validating the models written in PyTorch, one needs to write a training loop, paying particular attention to backpropagation. At the same time metrics should be computed in order to be able to make sense of the quality of the model.\n",
        "\n",
        "In order to simplify training and validation of models in PyTorch, I decided to create my own Trainer package in Python that would enable doing minumum actions to launch training. Hence, I created two packages in my Github repository:\n",
        "\n",
        "*  `pytorch_trainer` => Python package with the `Trainer` class for training models coded in PyTorch;\n",
        "* `pytorch_models` => Additional package containing ready-to-use custom models I built: currently there are two models (my implementations) - `EDNet` (short for *Encoder Decoder Network*) and `ResNet` (a more lightweight analog of PyTorch's *ResNet* model with some custom additions).\n",
        "\n",
        "Let's firstly clone the repository with these packages and import them:"
      ],
      "metadata": {
        "id": "rL8ijsNot0Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from pytorch_trainer import Trainer\n",
        "  from pytorch_models import ednet4, resnet20\n",
        "except ImportError:\n",
        "  !git clone https://github.com/spolivin/cifar10-website.git\n",
        "  %cd cifar10-website/nn_dev\n",
        "  from pytorch_trainer import Trainer\n",
        "  from pytorch_models import ednet4, resnet20\n",
        "  %cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjXGudqVksrc",
        "outputId": "33ab7970-ff24-41e6-aed4-fd5930211da7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cifar10-website'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 52 (delta 19), reused 39 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (52/52), 17.60 KiB | 8.80 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "/content/cifar10-website/nn_dev\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's import the other required libraries for running this notebook:"
      ],
      "metadata": {
        "id": "z6RHtXbEaHfa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tdPFV6eSDMQc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Setting batch size for PyTorch's Dataloaders\n",
        "BATCH_SIZE = 128\n",
        "# Setting means and st deviations for normalizing image data\n",
        "MEANS_NORMALIZATION =  [0.4914, 0.4822, 0.4465]\n",
        "STDEVS_NORMALIZATION = [0.247,  0.243,  0.261]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "SfRWD5FhaMCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can move to preprocessing the image dataset. Firstly, let's define which transformations we will be applying to training, validation and test sets:"
      ],
      "metadata": {
        "id": "-mT_DYwIxGYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HAJvkI4IDMQe"
      },
      "outputs": [],
      "source": [
        "# Transformations for training data\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        # Data Augmentation\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        # Conversion to tensor\n",
        "        transforms.ToTensor(),\n",
        "        # Normalization\n",
        "        transforms.Normalize(mean=MEANS_NORMALIZATION, std=STDEVS_NORMALIZATION),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Transformations for validation/test data\n",
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=MEANS_NORMALIZATION, std=STDEVS_NORMALIZATION),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we load the data with specifying the transformations we want to apply."
      ],
      "metadata": {
        "id": "bCT8kSZrxqfy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2ygvU70DMQe",
        "outputId": "0e801f9d-49d1-4d71-aad8-064464d8c398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Downloading training data\n",
        "training_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    transform=train_transform,\n",
        "    download=True,\n",
        ")\n",
        "\n",
        "# Downloading test data\n",
        "testing_set = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    transform=test_transform,\n",
        "    download=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to divide the dataset into batches using PyTorch's Dataloaders:"
      ],
      "metadata": {
        "id": "0CNFBLxMyAFN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PW0j9oMiDMQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "effd5a40-96e0-4bef-971e-f5e7dafe4a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: Dataset (50000 examples), Dataloader (391 batches)\n",
            "Validation set: Dataset (9000 examples), Dataloader (71 batches)\n",
            "Testing set: Dataset (1000 examples), Dataloader (8 batches)\n"
          ]
        }
      ],
      "source": [
        "# Dividing test set into validation and test sets\n",
        "validation_set = torch.utils.data.Subset(testing_set, torch.arange(1000, len(testing_set))) # The other testing examples -> validation set\n",
        "testing_set_2 = torch.utils.data.Subset(testing_set, torch.arange(1000)) # The first 1000 testing examples -> new test set\n",
        "\n",
        "# Create data loaders for the datasets\n",
        "training_dataloader = torch.utils.data.DataLoader(\n",
        "    training_set, batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "validation_dataloader = torch.utils.data.DataLoader(\n",
        "    validation_set, batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "testing_dataloader = torch.utils.data.DataLoader(\n",
        "    testing_set_2, batch_size=BATCH_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Training set: Dataset ({len(training_dataloader.dataset)} examples), Dataloader ({len(training_dataloader)} batches)\")\n",
        "print(f\"Validation set: Dataset ({len(validation_dataloader.dataset)} examples), Dataloader ({len(validation_dataloader)} batches)\")\n",
        "print(f\"Testing set: Dataset ({len(testing_dataloader.dataset)} examples), Dataloader ({len(testing_dataloader)} batches)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmtvy2Y4DMQf"
      },
      "source": [
        "## Application of Custom Trainer for PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will see the functionality of the trainer mentioned above. We will train the models in two specifications:\n",
        "\n",
        "* Encoder-Decoder\n",
        "* ResNet"
      ],
      "metadata": {
        "id": "X0TukuxS0y8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder-Decoder model - `EDNet`"
      ],
      "metadata": {
        "id": "Xm-9pIzw0rR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use `ednet4()` function from `pytorch_models` in order to load the prebuilt Encoder-Decoder model with 4 layers:"
      ],
      "metadata": {
        "id": "q3vnSVvq1BBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uBZS-q2KDMQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8edec28-3a67-4a01-e98f-5cc2c3c37369"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EDNet(\n",
              "  (encoder): Encoder(\n",
              "    (encoder_blocks): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU()\n",
              "        (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU()\n",
              "        (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (decoder_blocks): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): Linear(in_features=576, out_features=250, bias=True)\n",
              "        (1): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (last): Linear(in_features=250, out_features=10, bias=True)\n",
              "  )\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "encoder_decoder_model = ednet4()\n",
        "encoder_decoder_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set the loss function appropriate for the image classification problem at hand as well as the optimizer:"
      ],
      "metadata": {
        "id": "98Ez960E1WuR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-GCA50yVDMQg"
      },
      "outputs": [],
      "source": [
        "# Defining the loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining optimizer\n",
        "optimizer_params = {\n",
        "    \"lr\": 0.25,\n",
        "    \"momentum\": 0.75,\n",
        "}\n",
        "optimizer = torch.optim.SGD(encoder_decoder_model.parameters(), **optimizer_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can instantiate an object of `Trainer` class by specifying:\n",
        "\n",
        "* Built model => `model`\n",
        "* Loss function => `criterion`\n",
        "* Optimizer => `optimizer`\n",
        "* Training dataloader => `train_loader`\n",
        "* Validation dataloader => `valid_loader`\n",
        "* CPU or GPU-based training => `train_on`"
      ],
      "metadata": {
        "id": "lSa7FRaS1tH5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xtc6nckPDMQg"
      },
      "outputs": [],
      "source": [
        "trainer_ed = Trainer(\n",
        "    model=encoder_decoder_model,\n",
        "    criterion=loss_func,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=training_dataloader,\n",
        "    valid_loader=validation_dataloader,\n",
        "    train_on=\"cuda\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We would like to be able to adjust the learning rate during training using Torch scheduler. We can also use the Trainer to do this and in order to be able to tell it to apply scheduler on a certain level, we do the following:"
      ],
      "metadata": {
        "id": "ayRS-jcj2RmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting scheduler\n",
        "scheduler_params = {\n",
        "    \"step_size\": 4,\n",
        "    \"gamma\": 0.5,\n",
        "}\n",
        "trainer_ed.set_scheduler(\n",
        "    scheduler_class=torch.optim.lr_scheduler.StepLR,\n",
        "    scheduler_params=scheduler_params,\n",
        "    level=\"epoch\",\n",
        ")"
      ],
      "metadata": {
        "id": "g7bBHPdI2J68"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can finally launch the trainer. We will train during 10 epochs with a set seed of 42 for random number generator and a progress bar that would show the status of training and validation. At the end of each validation step we will see loss and accuracy in the following format:\n",
        "\n",
        "`loss=(  {train_loss}, {valid_loss}  )`\n",
        "\n",
        "`acc=(  {train_acc}, {valid_acc}  )`"
      ],
      "metadata": {
        "id": "VRyvHWWC20sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hr06JROhDMQg",
        "outputId": "37223a3b-aa65-4a7d-d1f3-f615f0e53038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Training]: 100%|██████████| 391/391 [00:23<00:00, 16.44batches/s, lr=0.25]\n",
            "Epoch 1/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 24.89batches/s, loss=(1.678, 1.4434), acc=(0.3788, 0.4912)]\n",
            "Epoch 2/10 [Training]: 100%|██████████| 391/391 [00:23<00:00, 16.59batches/s, lr=0.25]\n",
            "Epoch 2/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 29.64batches/s, loss=(1.2959, 1.093), acc=(0.5341, 0.6143)]\n",
            "Epoch 3/10 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.20batches/s, lr=0.25]\n",
            "Epoch 3/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 30.18batches/s, loss=(1.1408, 1.0461), acc=(0.5955, 0.6283)]\n",
            "Epoch 4/10 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.25batches/s, lr=0.25]\n",
            "Epoch 4/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 29.77batches/s, loss=(1.0663, 0.9913), acc=(0.6232, 0.6514)]\n",
            "Epoch 5/10 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.03batches/s, lr=0.125]\n",
            "Epoch 5/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 29.21batches/s, loss=(0.9238, 0.8413), acc=(0.6745, 0.7037)]\n",
            "Epoch 6/10 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.12batches/s, lr=0.125]\n",
            "Epoch 6/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 29.83batches/s, loss=(0.8828, 0.8113), acc=(0.6882, 0.7228)]\n",
            "Epoch 7/10 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.31batches/s, lr=0.125]\n",
            "Epoch 7/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 30.25batches/s, loss=(0.861, 0.8077), acc=(0.6996, 0.7154)]\n",
            "Epoch 8/10 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.19batches/s, lr=0.125]\n",
            "Epoch 8/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 29.25batches/s, loss=(0.8401, 0.7861), acc=(0.7047, 0.7254)]\n",
            "Epoch 9/10 [Training]: 100%|██████████| 391/391 [00:23<00:00, 16.99batches/s, lr=0.0625]\n",
            "Epoch 9/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 29.45batches/s, loss=(0.7735, 0.7546), acc=(0.7315, 0.7351)]\n",
            "Epoch 10/10 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.10batches/s, lr=0.0625]\n",
            "Epoch 10/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 29.84batches/s, loss=(0.7468, 0.7214), acc=(0.7394, 0.7504)]\n"
          ]
        }
      ],
      "source": [
        "trainer_ed.run(\n",
        "    epochs=10,\n",
        "    seed=42,\n",
        "    enable_tqdm=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the progress of the training, we have managed to reach about 75% on the validation set. Let's save this checkpoint in order be able to quickly come back to it if necessary. Method `save_checkpoint()` will save the last trained epoch, model state and optimizer state."
      ],
      "metadata": {
        "id": "RkyC6pd8sy-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_ed.save_checkpoint(path=\"ednet4_ckpt_10.pt\")"
      ],
      "metadata": {
        "id": "w3W77vqzKoVc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Continuation of training of ED Net"
      ],
      "metadata": {
        "id": "qYplTvzYJ4OP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we will see metrics improvements if we continue training for 5 additional epochs without scheduler:"
      ],
      "metadata": {
        "id": "hEapX9WPtW7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turning off scheduler\n",
        "trainer_ed.reset_scheduler()"
      ],
      "metadata": {
        "id": "81CijT5QJ7wu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When continuing training we will see that the trainer is able to remember the last trained epoch and conveniently show the consequent epochs in a progress bar:"
      ],
      "metadata": {
        "id": "-wpNnQZotjBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_ed.run(\n",
        "    epochs=5,\n",
        "    seed=42,\n",
        "    enable_tqdm=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wuHvZ7rJ7pE",
        "outputId": "00998962-e7e6-4df1-c851-5fec75983a75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/15 [Training]: 100%|██████████| 391/391 [00:21<00:00, 17.90batches/s, lr=0.0625]\n",
            "Epoch 11/15 [Validation]: 100%|██████████| 71/71 [00:03<00:00, 23.03batches/s, loss=(0.7408, 0.7165), acc=(0.7415, 0.7517)]\n",
            "Epoch 12/15 [Training]: 100%|██████████| 391/391 [00:21<00:00, 17.96batches/s, lr=0.0625]\n",
            "Epoch 12/15 [Validation]: 100%|██████████| 71/71 [00:03<00:00, 22.86batches/s, loss=(0.7218, 0.7346), acc=(0.7497, 0.7466)]\n",
            "Epoch 13/15 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.57batches/s, lr=0.0625]\n",
            "Epoch 13/15 [Validation]: 100%|██████████| 71/71 [00:03<00:00, 23.23batches/s, loss=(0.7078, 0.6998), acc=(0.7532, 0.7582)]\n",
            "Epoch 14/15 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.72batches/s, lr=0.0625]\n",
            "Epoch 14/15 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 24.19batches/s, loss=(0.6889, 0.7082), acc=(0.759, 0.7547)]\n",
            "Epoch 15/15 [Training]: 100%|██████████| 391/391 [00:22<00:00, 17.57batches/s, lr=0.0625]\n",
            "Epoch 15/15 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 25.70batches/s, loss=(0.6852, 0.6983), acc=(0.7603, 0.757)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet model with 20 layers - `ResNet`"
      ],
      "metadata": {
        "id": "2Jd18oKebAb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now do exactly the same but with the second model:"
      ],
      "metadata": {
        "id": "vJ7c7YbN4sJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_20 = resnet20()\n",
        "resnet_20.eval()"
      ],
      "metadata": {
        "id": "HjxIJfEEbCUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c520ac-c7ca-41e3-8f33-d9037b526e96"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that we basically have 3 layers here where each one consists of 3 blocks with 2 convolution layers inside. In other words, we have 18 layers here, plus additional convolution layer at the very beginning and a FC layer at the end, thus totalling 20 layers.\n",
        "\n",
        "I decided to opt for this more lightweight model, since the out-of-the-box ResNet models from PyTorch (ResNet18 or ResNet54) are specifically tailored to image data of 224x224x3 and may not be suitable to CIFAR-10 dataset with images of 32x32x3. By using my implementation, we can save more information when going through convolution layers and achieve better results, hence potentially improving on Encoder-Decoder model above."
      ],
      "metadata": {
        "id": "Qg7m-upO419w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(resnet_20.parameters(), **optimizer_params)"
      ],
      "metadata": {
        "id": "0MTRCJq8bC12"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_resnet = Trainer(\n",
        "    model=resnet_20,\n",
        "    criterion=loss_func,\n",
        "    optimizer=optimizer,\n",
        "    train_loader=training_dataloader,\n",
        "    valid_loader=validation_dataloader,\n",
        "    train_on=\"cuda\",\n",
        ")\n",
        "\n",
        "trainer_resnet.set_scheduler(\n",
        "    scheduler_class=torch.optim.lr_scheduler.StepLR,\n",
        "    scheduler_params=scheduler_params,\n",
        "    level=\"epoch\",\n",
        ")"
      ],
      "metadata": {
        "id": "TtQP9UQMbCzX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_resnet.run(\n",
        "    epochs=10,\n",
        "    seed=42,\n",
        "    enable_tqdm=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdbhVuMzbCw6",
        "outputId": "c93d6e6e-cd8b-4183-88ff-ce4f1e231664"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Training]: 100%|██████████| 391/391 [00:33<00:00, 11.77batches/s, lr=0.25]\n",
            "Epoch 1/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 26.03batches/s, loss=(1.7261, 1.8801), acc=(0.3528, 0.385)]\n",
            "Epoch 2/10 [Training]: 100%|██████████| 391/391 [00:32<00:00, 12.02batches/s, lr=0.25]\n",
            "Epoch 2/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 26.39batches/s, loss=(1.3062, 1.2349), acc=(0.5244, 0.5636)]\n",
            "Epoch 3/10 [Training]: 100%|██████████| 391/391 [00:32<00:00, 12.11batches/s, lr=0.25]\n",
            "Epoch 3/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 26.59batches/s, loss=(1.0489, 1.0369), acc=(0.6277, 0.6384)]\n",
            "Epoch 4/10 [Training]: 100%|██████████| 391/391 [00:32<00:00, 11.91batches/s, lr=0.25]\n",
            "Epoch 4/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 25.35batches/s, loss=(0.8597, 1.081), acc=(0.6977, 0.6583)]\n",
            "Epoch 5/10 [Training]: 100%|██████████| 391/391 [00:33<00:00, 11.64batches/s, lr=0.125]\n",
            "Epoch 5/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 24.72batches/s, loss=(0.6789, 0.6706), acc=(0.7642, 0.7731)]\n",
            "Epoch 6/10 [Training]: 100%|██████████| 391/391 [00:32<00:00, 11.99batches/s, lr=0.125]\n",
            "Epoch 6/10 [Validation]: 100%|██████████| 71/71 [00:03<00:00, 21.23batches/s, loss=(0.6188, 0.6704), acc=(0.7848, 0.7753)]\n",
            "Epoch 7/10 [Training]: 100%|██████████| 391/391 [00:32<00:00, 12.12batches/s, lr=0.125]\n",
            "Epoch 7/10 [Validation]: 100%|██████████| 71/71 [00:03<00:00, 20.43batches/s, loss=(0.5772, 0.6196), acc=(0.799, 0.7907)]\n",
            "Epoch 8/10 [Training]: 100%|██████████| 391/391 [00:32<00:00, 12.11batches/s, lr=0.125]\n",
            "Epoch 8/10 [Validation]: 100%|██████████| 71/71 [00:03<00:00, 22.88batches/s, loss=(0.5441, 0.6048), acc=(0.8117, 0.799)]\n",
            "Epoch 9/10 [Training]: 100%|██████████| 391/391 [00:32<00:00, 11.95batches/s, lr=0.0625]\n",
            "Epoch 9/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 25.39batches/s, loss=(0.4693, 0.5808), acc=(0.838, 0.8109)]\n",
            "Epoch 10/10 [Training]: 100%|██████████| 391/391 [00:32<00:00, 12.04batches/s, lr=0.0625]\n",
            "Epoch 10/10 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 25.66batches/s, loss=(0.4443, 0.5668), acc=(0.8464, 0.8153)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_resnet.save_checkpoint(\"resnet20_ckpt_10.pt\")"
      ],
      "metadata": {
        "id": "9C5_XtdVHQ97"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Continuation of training of ResNet-20"
      ],
      "metadata": {
        "id": "8dp14zGtEkHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_resnet.reset_scheduler()"
      ],
      "metadata": {
        "id": "vbnwmTlbBnfm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_resnet.run(\n",
        "    epochs=5,\n",
        "    seed=42,\n",
        "    enable_tqdm=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6ihBpJ6Bqqj",
        "outputId": "79f135c0-277e-4566-f678-984035189e64"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/15 [Training]: 100%|██████████| 391/391 [00:32<00:00, 12.02batches/s, lr=0.0625]\n",
            "Epoch 11/15 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 25.85batches/s, loss=(0.4295, 0.5848), acc=(0.8506, 0.8039)]\n",
            "Epoch 12/15 [Training]: 100%|██████████| 391/391 [00:32<00:00, 11.95batches/s, lr=0.0625]\n",
            "Epoch 12/15 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 25.19batches/s, loss=(0.4203, 0.4963), acc=(0.8538, 0.8368)]\n",
            "Epoch 13/15 [Training]: 100%|██████████| 391/391 [00:32<00:00, 11.88batches/s, lr=0.0625]\n",
            "Epoch 13/15 [Validation]: 100%|██████████| 71/71 [00:02<00:00, 25.53batches/s, loss=(0.4001, 0.5117), acc=(0.8606, 0.8353)]\n",
            "Epoch 14/15 [Training]: 100%|██████████| 391/391 [00:32<00:00, 12.17batches/s, lr=0.0625]\n",
            "Epoch 14/15 [Validation]: 100%|██████████| 71/71 [00:03<00:00, 21.51batches/s, loss=(0.3869, 0.5834), acc=(0.8654, 0.8092)]\n",
            "Epoch 15/15 [Training]: 100%|██████████| 391/391 [00:31<00:00, 12.26batches/s, lr=0.0625]\n",
            "Epoch 15/15 [Validation]: 100%|██████████| 71/71 [00:03<00:00, 20.72batches/s, loss=(0.3757, 0.4988), acc=(0.8683, 0.8393)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "XmGpXlCQ7zDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the result of trainings above, we have determined that ResNet-20 model significantly outperforms EDNet model. Let's now check both models on test sets:"
      ],
      "metadata": {
        "id": "M2w6Kepux_hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad\n",
        "def test_torch_model(\n",
        "    trainer: Trainer,\n",
        "    dataloader: torch.utils.data.DataLoader = testing_dataloader,\n",
        "    device: str = \"cuda\",\n",
        ") -> dict[str, float]:\n",
        "    \"\"\"Tests the model saved as Trainer attribute on a set.\n",
        "\n",
        "    Args:\n",
        "      trainer (Trainer): Instance of Trainer class.\n",
        "      dataloader (torch.utils.data.DataLoader, optional): Torch Dataloader.\n",
        "      device (str, optional): Indicator of CPU or CUDA-based testing.\n",
        "\n",
        "    Returns:\n",
        "      dict[str, float]: Dict of loss and accuracy.\n",
        "    \"\"\"\n",
        "    # Setting counters\n",
        "    test_loss = 0\n",
        "    correct_ans = 0\n",
        "    for _, batch in enumerate(tqdm(dataloader)):\n",
        "      # Separating batch into examples and labels + moving to device\n",
        "      x_batch, y_batch = batch\n",
        "      x_batch, y_batch = x_batch.to(torch.device(device)), y_batch.to(torch.device(device))\n",
        "      # Computing batch loss and outputs of the last layer\n",
        "      loss, outputs = trainer(x_batch, y_batch)\n",
        "\n",
        "      # Adding loss to counter\n",
        "      test_loss += loss\n",
        "      # Computing number of correct predictions\n",
        "      predictions = torch.argmax(outputs, dim=1)\n",
        "      correct = (predictions == y_batch).sum().cpu().item()\n",
        "      correct_ans += correct\n",
        "\n",
        "    # Computing epoch-level metrics\n",
        "    test_loss /= len(dataloader)\n",
        "    test_loss = test_loss.item()\n",
        "    test_accuracy = correct_ans / len(dataloader.dataset)\n",
        "\n",
        "    test_metrics = {\n",
        "        \"test_loss\": round(test_loss, 4),\n",
        "        \"test_accuracy\": round(test_accuracy, 4)\n",
        "    }\n",
        "\n",
        "    return test_metrics"
      ],
      "metadata": {
        "id": "DW0O2UwP71Go"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_torch_model(trainer=trainer_ed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMVMFxvUx1W6",
        "outputId": "5e28c95a-e910-4107-ed37-971f3640535d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 29.90it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.6721, 'test_accuracy': 0.774}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_torch_model(trainer=trainer_resnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHMbumgXIHod",
        "outputId": "55c938e4-c26a-4bee-a00e-ba7c0134db50"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 21.19it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 0.4681, 'test_accuracy': 0.848}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving best model weights"
      ],
      "metadata": {
        "id": "mecEyHWPAfgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = trainer_resnet.model\n",
        "\n",
        "torch.save(best_model.state_dict(), \"resnet20_weights.pth\")"
      ],
      "metadata": {
        "id": "dMHnpKJJ70_h"
      },
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}